{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import gc\n",
    "from IPython.display import display, Markdown \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    OFFLINE = False #True # for Test offline environment\n",
    "    USE_LLAMA3 = False # for GPU version\n",
    "    USE_GAMMA = True # for GPU version\n",
    "    USE_GEMMA2 = False # for GPU version only \n",
    "    TASK_GEN = True # for generative Text output task (suitable for RAG project)\n",
    "    model1 = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # llama3 8B\n",
    "    model2 =  \"google/gemma-1.1-2b-it\" #  gemma 2B\n",
    "    model3 = \"google/gemma-7b-it\"# gemma 7B\n",
    "    model4 =  \"google/gemma-2-9b-it\" # gemma 2 9B\n",
    "    model5 =  'yentinglin/Llama-3-Taiwan-8B-Instruct' # Chinese version of llama3\n",
    "    model6 = 'Qwen/Qwen-7B' # support Chinese version \n",
    "    model7 = \"THUDM/chatglm-6b\" # support Chinese version\n",
    "    embedModel1 = 'intfloat/multilingual-e5-small' # for embedding model support chinese\n",
    "    embedModel2 = \"all-MiniLM-L6-v2\"\n",
    "    embedModel3 = \"BAAI/bge-base-en-v1.5\" # for embedding model support chinese\n",
    "    embedModel4 = \"BAAI/bge-m3\" # for multilingual embedding model\n",
    "    \n",
    "\n",
    "\n",
    "    FEW_SHOT_TEST= False#True\n",
    "    USE_RAG = False#False#False #True#True , in this project, prefer use fine tuning for p\n",
    "    USE_WANDB = True#True # for  LLM evalution and debug , track fine tuning performance\n",
    "    USE_TRULENS = False # for LLM evalution For RAG prefer \n",
    "    USE_DEEPEVAL = False # for LLM evalution   (require openAI API key)\n",
    "    USE_TRAIN =  True #True #False#True Much be use GPU for Training \n",
    "    loggingSteps= 10#100 #100, #20, #5,#10,\n",
    "    maxTrainData = 200#3500#5000 #10000#5000 #10000\n",
    "    maxEvalData = 20#100 # 20 \n",
    "    maxToken=  512#768#512#768 # 512 for test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 14:59:26.752519: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-21 14:59:26.973749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 14:59:27.084258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 14:59:27.085037: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 14:59:27.250704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 14:59:27.986567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                         AutoModelForCausalLM,\n",
    "                         TrainingArguments)\n",
    "\n",
    "from langchain_community.document_loaders import (TextLoader,\n",
    "                                                PyPDFLoader)\n",
    "\n",
    "from langchain.prompts.prompt import  PromptTemplate\n",
    "from langchain_community.vectorstores import (FAISS, \n",
    "                                              Chroma \n",
    "                                              )\n",
    "\n",
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter,\n",
    "                                      CharacterTextSplitter ,\n",
    "                                       SentenceTransformersTokenTextSplitter)   \n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "\n",
    "import evaluate\n",
    "import trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearMemory():\n",
    "    for _ in range(5):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HuggingFace Hub Access for download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "huggingfaceToken = os.getenv(\"HuggingFace\") #get huggeface token from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "if CFG.USE_WANDB:\n",
    "    # train report to  W&B tool\n",
    "    import wandb\n",
    "    reportTo= \"wandb\"\n",
    "    my_secret = os.getenv(\"wandb_api_key\") \n",
    "    wandb.login(key=my_secret) # login \n",
    "else: \n",
    "    reportTo = \"none\"# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized Config for GPU support only\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(CFG.model2, token=huggingfaceToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afa3c1337e54f548d6ffe8d84cda22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea278afa761f4c9b86d0299c44e3ead9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b29f9f0b35d448789919aeac6635a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb629956e6f46a6bda53a3347aea463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1230b704c7b64da595d91ef5530145d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92acfa5502d04219b48104e0a73333c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3480cacd8641a2830031ab5133c847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d242bf38debc41a780da21a6c9c936c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9488fa4b8c040da87f639a069e7fab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e751ff8687ad4ac2846b7635cb3553d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6af29575e14ca1a125492334431d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba4cc4eb13b446d8faea498bb6f2c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a92bb36a78042d4ae301004b431cbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if device.type == \"cuda\": # use 7b/8b/9b model gain performance\n",
    "    if CFG.USE_LLAMA3:\n",
    "        modelSel = CFG.model1\n",
    "        llmModel = \"llama3_8b\"\n",
    "        \n",
    "    elif CFG.USE_GEMMA2:\n",
    "        modelSel = CFG.model4\n",
    "        llmModel = \"gemma2_9b\"\n",
    "    \n",
    "    elif CFG.USE_GAMMA:\n",
    "        modelSel = CFG.model3\n",
    "        llmModel = \"gemma_7b\"\n",
    "    else:\n",
    "        modelSel = CFG.model2\n",
    "        llmModel = 'gemma_2b'\n",
    "        \n",
    "    if CFG.TASK_GEN:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  \n",
    "                                                 quantization_config= bnb_config ,\n",
    "                                                 token=huggingfaceToken)\n",
    "\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  \n",
    "                                                 quantization_config= bnb_config, token=huggingfaceToken)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelSel, token=huggingfaceToken) # inital tokenizer\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "\n",
    "else: # for cpu select smaller model\n",
    "    modelSel = CFG.model2\n",
    "    llmModel = 'gemma_2b'\n",
    "    if CFG.TASK_GEN:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\", token=huggingfaceToken)\n",
    "\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\", token=huggingfaceToken)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelSel, token=huggingfaceToken) # inital tokenizer\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma_7b'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "templatePrompt1 = \"\"\"Question: {question}.\\nOnly require given final result in JSON format with key 'answer'\n",
    "            \"\"\"\n",
    "templatePrompt2 = \"Answer the user Question.\\n###\\n{format_instructions}\\n###\\nQuestion: {query}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: {question}.\\nOnly require given final result in JSON format with key 'answer'\\n            \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(query, maxOutToken = 512):\n",
    "    \"\"\"\n",
    "    Direct send message to LLM model, get response\n",
    "    \"\"\"\n",
    "    inputIds = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    response = model.generate(**inputIds,\n",
    "                              do_sample=True,\n",
    "                              top_p=0.95,\n",
    "                              top_k = 3,\n",
    "                              temperature=0.5,\n",
    "                              max_new_tokens= maxOutToken,\n",
    "                             )\n",
    "    return tokenizer.decode(response[0][len(inputIds[\"input_ids\"]):], skip_special_tokens = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple parser for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from  json.decoder import JSONDecodeError\n",
    "if CFG.TASK_GEN:\n",
    "\n",
    "    def isInteger(text):\n",
    "        try:\n",
    "            if int(text) >= 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def llmJSONparser(txt, key=\"answer\"):\n",
    "        \"\"\"\n",
    "        try to get answer from LLM response , expect in JSON format, \n",
    "        \"\"\"\n",
    "        try:\n",
    "            subText = txt.split(\"{\") # split several {} in list \n",
    "            for txtSeg in subText: # loop in list to find answer\n",
    "                end = txtSeg.find(\"}\") # find end position in text segment\n",
    "                sub = txtSeg[:end] #subsring with {} context\n",
    "                print(sub)\n",
    "                temp = sub.replace(\"*\", \"\") # remove * symbol\n",
    "                temp = temp.replace(\"\\\"\", \"\") # reomve \\\" symbol\n",
    "                temp = temp.lower() # convert to lower case\n",
    "                answerloc = temp.find(key) # find key word \"answer\" position\n",
    "                if answerloc != -1:\n",
    "                    print(f\"find answer location : {answerloc}\")\n",
    "                    newTxt = temp[answerloc:] # substring start answer\n",
    "#                   print(\"Temp: \", temp)\n",
    "                    subTxt = newTxt.split(\"\\n\")\n",
    "                    #       print(subTxt)\n",
    "                    rel =subTxt[0][len(key):].strip() # get answer value with remove space\n",
    "                    rel= rel.replace(',', '') # remove , symbol\n",
    "                    print(rel)\n",
    "                    return rel\n",
    "                \n",
    "            return None # can't find answer\n",
    "        except :\n",
    "            print(f\"\"\"Error LLM JSON parser input txt {txt}\"\"\" )\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "\n",
    "    def getLLMAnswerParser(txt, key=\"answer:\"):\n",
    "        \"\"\"\n",
    "        when json parser failure, seem answer not JSON format, \n",
    "        use \"answer\" for key word search final answer \n",
    "        \"\"\"\n",
    "         # find answer  \n",
    "        temp = txt.replace(\"*\", \"\") # remove * symbol\n",
    "        temp = temp.replace(\"\\\"\", \"\") # reomve \"\" symbol\n",
    "        temp = temp.lower() # convert to lower case\n",
    "        # find answer key word\n",
    "        start = temp.find(key)\n",
    "        print(f\"Start loc: {start}\")\n",
    "        subStr = temp[start:]\n",
    "        if start != -1:\n",
    "            subTxt = subStr.split(\"\\n\")\n",
    "           #print(subTxt)\n",
    "            rel =subTxt[0][len(key):].strip() # get answer value with remove space\n",
    "            rel= rel.replace(',', '') # remove , symbol\n",
    "            print(rel)\n",
    "            return rel\n",
    "    \n",
    "        print(subStr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add parser  to control extreact data from  LLM Structure Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import (StrOutputParser, \n",
    "                                           JsonOutputParser,\n",
    "                                           PydanticOutputParser,\n",
    "                                          )\n",
    "# for LLM structure output\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "# from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.13 s, sys: 0 ns, total: 6.13 s\n",
      "Wall time: 6.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ret =generateResponse(\"What is Machine Learning?\" , maxOutToken=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Machine Learning?\n",
      "\n",
      "Machine learning is a field of computer science that enables computers to learn from data, identify patterns, and make predictions. It involves teaching computers to learn from experience, rather than being explicitly programmed.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "* **Learning:** The process of a computer acquiring knowledge from data.\n",
      "* **Algorithm:** A set of rules or instructions that guide the learning process.\n",
      "* **Model:** A representation of the learned knowledge that can be used to make predictions or decisions.\n",
      "* **Data:** The raw material that is used for learning.\n",
      "* **Supervised Learning:** Learning from labeled data, where the computer is shown examples and learns from them.\n",
      "* **Unsupervised Learning:** Learning from unlabeled data, where the computer discovers patterns without any examples.\n",
      "\n",
      "**Types of Learning:**\n",
      "\n",
      "* **Supervised Learning:** Involves learning from labeled data, where the computer is shown examples and learns from them.\n",
      "* **Unsupervised Learning:** Involves learning from unlabeled data, where the computer discovers patterns without any examples.\n",
      "* **Reinforcement Learning:** Involves learning through trial and error, where the computer learns by interacting with its environment.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Machine learning has a wide range of applications in various fields, including\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What is Machine Learning?\n",
       "\n",
       "Machine learning is a field of computer science that enables computers to learn from data, identify patterns, and make predictions. It involves teaching computers to learn from experience, rather than being explicitly programmed.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Learning:** The process of a computer acquiring knowledge from data.\n",
       "* **Algorithm:** A set of rules or instructions that guide the learning process.\n",
       "* **Model:** A representation of the learned knowledge that can be used to make predictions or decisions.\n",
       "* **Data:** The raw material that is used for learning.\n",
       "* **Supervised Learning:** Learning from labeled data, where the computer is shown examples and learns from them.\n",
       "* **Unsupervised Learning:** Learning from unlabeled data, where the computer discovers patterns without any examples.\n",
       "\n",
       "**Types of Learning:**\n",
       "\n",
       "* **Supervised Learning:** Involves learning from labeled data, where the computer is shown examples and learns from them.\n",
       "* **Unsupervised Learning:** Involves learning from unlabeled data, where the computer discovers patterns without any examples.\n",
       "* **Reinforcement Learning:** Involves learning through trial and error, where the computer learns by interacting with its environment.\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "Machine learning has a wide range of applications in various fields, including"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(ret)) # display in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 885 ms, sys: 25.4 ms, total: 911 ms\n",
      "Wall time: 909 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"What is Machine Learning?\"\n",
    "newPrompt = PromptTemplate(template=templatePrompt1,\n",
    "                           input_variables=[\"question\"])\n",
    "finalPrompt = newPrompt.format(\n",
    "                question=query    \n",
    "            )\n",
    "rel =generateResponse(finalPrompt, maxOutToken=1024)\n",
    "# jsonTxt = getLLMAnswerParser(rel, key=\"answer\")\n",
    "# print(f\"Question : {query}\\nResponse Answer: {jsonTxt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Machine Learning?.\n",
      "Only require given final result in JSON format with key 'answer'\n",
      "            {\n",
      "                \"answer\": \"Machine learning is a field of computer science that enables computers to learn from data, identify patterns, and make predictions.\"\n",
      "            }\n"
     ]
    }
   ],
   "source": [
    "print(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare RAG "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
